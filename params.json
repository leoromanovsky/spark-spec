{"name":"Spark Spec","tagline":"Testing utilities for Apache Spark.","body":"# Spark Spec\r\n\r\nThis project is a collection of utility classes to make ease testing Apache Spark programs.\r\n\r\n```\r\nclass SimpleExampleSpec extends SparkSpecUtils with ShouldMatchers {\r\n  sparkTest(\"spark parallelize\") {\r\n    val data = sc.parallelize(1 to 1e6.toInt)\r\n    val result = data.filter{_ % 2 == 0}.count\r\n    println(\"Result:\", result)\r\n    result should be (5e5.toInt)\r\n  }\r\n}\r\n\r\n> test-only com.strava.discovery.SimpleSparkTest\r\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\n15/03/01 12:02:05 INFO SecurityManager: Changing view acls to: foo\r\n...\r\n...\r\n15/03/01 12:02:07 INFO DAGScheduler: Stage 0 (count at SimpleSparkTest.scala:9) finished in 0.095 s\r\n15/03/01 12:02:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool\r\n15/03/01 12:02:07 INFO SparkContext: Job finished: count at SimpleSparkTest.scala:9, took 0.206779 s\r\n(Result:,500000)\r\n15/03/01 12:02:07 INFO SparkUI: Stopped Spark web UI at http://192.168.8.103:4040\r\n15/03/01 12:02:07 INFO DAGScheduler: Stopping DAGScheduler\r\n15/03/01 12:02:08 INFO MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!\r\n15/03/01 12:02:08 INFO ConnectionManager: Selector thread was interrupted!\r\n15/03/01 12:02:08 INFO ConnectionManager: ConnectionManager stopped\r\n15/03/01 12:02:08 INFO MemoryStore: MemoryStore cleared\r\n15/03/01 12:02:08 INFO BlockManager: BlockManager stopped\r\n15/03/01 12:02:08 INFO BlockManagerMaster: BlockManagerMaster stopped\r\n15/03/01 12:02:08 INFO SparkContext: Successfully stopped SparkContext\r\n[info] SimpleSparkTest:\r\n[info] - spark filter\r\n[info] Run completed in 3 seconds, 465 milliseconds.\r\n[info] Total number of tests run: 1\r\n[info] Suites: completed 1, aborted 0\r\n[info] Tests: succeeded 1, failed 0, canceled 0, ignored 0, pending 0\r\n[info] All tests passed.\r\n[success] Total time: 4 s, completed Mar 1, 2015 12:02:08 PM\r\n```\r\n\r\nThe Spark cluster was launched in local mode behind the scenes, kept around for the duration of the specs in the \r\nclass, and shutdown afterwards.\r\n\r\nI've divided it into two directories:\r\n\r\n* core - The library which a developer includes into their project.\r\n* examples - A standalone sbt project meant to be fully transferable.\r\n\r\n## Getting Started\r\n\r\nIn your Build.scala: \r\n\r\n```\r\nlibraryDependencies ++= Seq(\"com.leoromanovsky\" %% \"spark-spec-core\" % \"0.0.1-SNAPSHOT\" % \"test\")\r\n```\r\n\r\nAdd a spec to test `MyETLJob` class:\r\n\r\n```\r\npackage com.strava.discovery.etl\r\n\r\nimport com.leoromanovsky.sparkspec.core.SparkSpecUtils\r\nimport org.scalatest.ShouldMatchers\r\nimport com.foo.etl.MyETLJob\r\n\r\nclass MyETLJobSpec extends SparkSpecUtils with ShouldMatchers {\r\n  sparkTest(\"my etl\") {\r\n    val results = MyETLJob.runJob(sc, config)\r\n    println(\"Result:\", results)\r\n    results should be (1)\r\n  }\r\n}\r\n```\r\n\r\nThe test passed in a `SparkContext` and `Config` as `sc` and `config`, respectfully.\r\n\r\n## Contributing\r\n\r\nI welcome feedback, issue reports and pull requests.\r\n\r\nLeo\r\n","google":"UA-60245074-1","note":"Don't delete this file! It's used internally to help with page regeneration."}